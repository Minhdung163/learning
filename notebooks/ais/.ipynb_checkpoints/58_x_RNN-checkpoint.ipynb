{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2064bc8-d7c7-412f-9a70-cfe9ea4bf2ea",
   "metadata": {},
   "source": [
    "# RNN (recurrent neural network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663d7da4-d9f3-4bfd-b82c-443ea8846c79",
   "metadata": {},
   "source": [
    "## Syntetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66444666-69f9-4b1d-8b22-3424767f2f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "num_samples = 10\n",
    "sequence_length = 4\n",
    "\n",
    "# Generate a synthetic time series dataset\n",
    "def generate_sequence(start_value, length):\n",
    "    sequence = [start_value]\n",
    "    for _ in range(length - 1):\n",
    "        next_value = sequence[-1] + np.random.normal(0, 0.1)\n",
    "        sequence.append(next_value)\n",
    "    return sequence\n",
    "\n",
    "# Create the dataset\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    start_value = np.random.uniform(-1, 1)\n",
    "    sequence = generate_sequence(start_value, sequence_length)\n",
    "    target_value = sequence[-1] + np.random.normal(0, 0.1)\n",
    "\n",
    "    X.append(sequence[:-1])  # Input sequence\n",
    "    y.append(target_value)    # Target value to predict\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd31ace1-21d5-46de-9177-0f19f0d363ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "727c32d3-2ce7-433c-a51a-610769675a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 08:44:15.295948: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 08:44:16.847303: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 436ms/step - loss: 1.3304\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0779\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8544\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6603\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4952\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3584\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2493\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1670\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1105\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0779\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0663\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0713\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0876\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1091\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1302\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1466\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1559\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1576\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1523\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1419\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1283\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1133\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0988\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0860\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0757\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0684\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0639\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0620\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0621\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0636\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0658\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0683\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0705\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0721\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0730\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0730\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0723\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0710\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0693\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0673\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0653\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0635\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0619\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0607\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0599\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0594\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0592\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0593\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0595\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb5c706920>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(units=64, activation='tanh', input_shape=(None, 1)),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X, y, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b5448e7-be2d-4fad-a02c-0b35a3f97d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.0599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05989203602075577"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e444e3-1292-4d65-b8da-3e99c32aad32",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec37e5-be94-4056-8d49-94de83235606",
   "metadata": {},
   "source": [
    "https://github.com/gabrielloye/RNN-walkthrough/blob/master/main.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9209d95-a232-4511-9c6f-4e7c77e3e2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b37b690b-9bb8-43bf-a2be-9cf659b42120",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['hey how are you','good i am fine','have a nice day']\n",
    "\n",
    "# Join all the sentences together and extract the unique characters from the combined sentences\n",
    "chars = set(''.join(text))\n",
    "\n",
    "# Creating a dictionary that maps integers to the characters\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "# Creating another dictionary that maps characters to integers\n",
    "char2int = {char: ind for ind, char in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b3d6dfe-c297-4200-a96d-0f144e5f6782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y': 0, 'i': 1, 'n': 2, 'e': 3, 'a': 4, 'u': 5, 'm': 6, 'w': 7, 'g': 8, 'v': 9, 'c': 10, 'r': 11, 'f': 12, ' ': 13, 'o': 14, 'h': 15, 'd': 16}\n"
     ]
    }
   ],
   "source": [
    "print(char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ff218d4-5c18-4248-8761-4a1c8b7e8106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest string has 15 characters\n"
     ]
    }
   ],
   "source": [
    "maxlen = len(max(text, key=len))\n",
    "print(\"The longest string has {} characters\".format(maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76613aa1-acf5-471e-b1ba-074ebf0d9462",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Padding\n",
    "\n",
    "# A simple loop that loops through the list of sentences and adds a ' ' whitespace until the length of the sentence matches\n",
    "# the length of the longest sentence\n",
    "for i in range(len(text)):\n",
    "    while len(text[i])<maxlen:\n",
    "        text[i] += ' '\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8d76399-ec5a-4914-abe2-ab36b51850fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: hey how are yo\n",
      "Target Sequence: ey how are you\n",
      "Input Sequence: good i am fine\n",
      "Target Sequence: ood i am fine \n",
      "Input Sequence: have a nice da\n",
      "Target Sequence: ave a nice day\n"
     ]
    }
   ],
   "source": [
    "# Creating lists that will hold our input and target sequences\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    # Remove last character for input sequence\n",
    "    input_seq.append(text[i][:-1])\n",
    "    \n",
    "    # Remove firsts character for target sequence\n",
    "    target_seq.append(text[i][1:])\n",
    "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c561e343-741d-448c-bf58-e370bee60a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08efbed6-d282-4733-860e-ae7a89126cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f80138-91d0-4fd8-9bc6-3e3874d02cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a792bcbf-3c3e-4800-b86d-beaef6d821ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a99d675-49e0-4008-9927-bcd94d34ad0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5e0e5cb-ea2f-4bc7-85bf-2a00db71ed18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (3, 14, 17) --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n"
     ]
    }
   ],
   "source": [
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
    "print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133e80a1-8500-4633-a4d1-11c93b0111b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bb2910d-435f-4f22-a804-eddb953fc100",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = torch.from_numpy(input_seq)\n",
    "target_seq = torch.Tensor(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d282af3a-9750-4150-a343-a72a78134e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55eccaf7-4c9c-40f8-949f-0eb0b7c832f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103903f9-1f4f-450a-8a1b-1c74cc8cd32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7378567e-7c59-49a6-93fa-72b330e98277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        #Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d9158-ee91-4240-b286-80dc7a4dca8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03f08f10-5702-4524-abc6-caeff416f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define hyperparameters\n",
    "n_epochs = 100\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370375d-7ef7-4fe7-9914-6b6c5d1690ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a733417-b3ee-45f6-81e6-85598b3f0e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100............. Loss: 2.3997\n",
      "Epoch: 20/100............. Loss: 2.1246\n",
      "Epoch: 30/100............. Loss: 1.7522\n",
      "Epoch: 40/100............. Loss: 1.3222\n",
      "Epoch: 50/100............. Loss: 0.9516\n",
      "Epoch: 60/100............. Loss: 0.6581\n",
      "Epoch: 70/100............. Loss: 0.4557\n",
      "Epoch: 80/100............. Loss: 0.3161\n",
      "Epoch: 90/100............. Loss: 0.2265\n",
      "Epoch: 100/100............. Loss: 0.1687\n"
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "input_seq = input_seq.to(device)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    #input_seq = input_seq.to(device)\n",
    "    output, hidden = model(input_seq)\n",
    "    output = output.to(device)\n",
    "    target_seq = target_seq.to(device)\n",
    "    loss = criterion(output, target_seq.view(-1).long())\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6fae98-4da8-4d10-8383-d7c3b6b30922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e26207a2-e764-46ad-91c9-9fd9edbeb0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, character):\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    character = np.array([[char2int[c] for c in character]])\n",
    "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
    "    character = torch.from_numpy(character)\n",
    "    character = character.to(device)\n",
    "    \n",
    "    out, hidden = model(character)\n",
    "\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    # Taking the class with the highest probability score from the output\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "    return int2char[char_ind], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4adaa5-a92f-4857-bf20-6c208a454d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f9ec3a6-103d-467e-86d3-411317e04e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, out_len, start='hey'):\n",
    "    model.eval() # eval mode\n",
    "    start = start.lower()\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(model, chars)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ab3e0-9d45-42e4-9e88-e1c7e81ad88e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74d87b01-8c38-4d8e-b4d0-f1e5f2547ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good i am fine '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(model, 15, 'good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e37a73-793c-487e-a58b-e0e912fdad1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf0237c1-ee5c-4575-9ef9-b9008b1d63ba",
   "metadata": {},
   "source": [
    "## Naive Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53437f31-8faa-4b5b-a9f6-6abafe2246e7",
   "metadata": {},
   "source": [
    "https://github.com/gabrielloye/RNN-walkthrough/blob/master/main.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f270e5a-84ce-4b55-bab7-2e2b60a7e369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_ .'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "allchars = string.ascii_letters + string.digits + \"_ .\"\n",
    "allchars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1988af7d-f43a-4c11-904c-bbbd5bb323eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cc32bea4-eb01-42fb-aecd-446c81398d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25, 'A': 26, 'B': 27, 'C': 28, 'D': 29, 'E': 30, 'F': 31, 'G': 32, 'H': 33, 'I': 34, 'J': 35, 'K': 36, 'L': 37, 'M': 38, 'N': 39, 'O': 40, 'P': 41, 'Q': 42, 'R': 43, 'S': 44, 'T': 45, 'U': 46, 'V': 47, 'W': 48, 'X': 49, 'Y': 50, 'Z': 51, '0': 52, '1': 53, '2': 54, '3': 55, '4': 56, '5': 57, '6': 58, '7': 59, '8': 60, '9': 61, '_': 62, ' ': 63, '.': 64}\n"
     ]
    }
   ],
   "source": [
    "text = {'hey how are you': 'hey how are you', 'good i am fine': 'good i am fine', 'have a nice day': 'have a nice day'}\n",
    "text = {'UserModel': 'UserGQLModel', 'GroupModel': 'GroupGQLModel', 'MembershipModel': 'MembershipGQLModel'}\n",
    "\n",
    "# Join all the sentences together and extract the unique characters from the combined sentences\n",
    "keys = list(text.keys())\n",
    "values = list(text.values())\n",
    "\n",
    "chars = set(''.join(keys).join(values))\n",
    "\n",
    "# Creating a dictionary that maps integers to the characters\n",
    "int2char = dict(enumerate(allchars))\n",
    "\n",
    "# Creating another dictionary that maps characters to integers\n",
    "char2int = {char: ind for ind, char in int2char.items()}\n",
    "print(char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456aaa11-e39d-45ab-9ec6-a83fe29e45ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "572ce828-fd39-480d-a4af-ee6e32799810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest string has 18 characters\n"
     ]
    }
   ],
   "source": [
    "maxlen = len(max(keys + values, key=len))\n",
    "print(\"The longest string has {} characters\".format(maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c8fe1a3d-92bf-4998-be7d-31b5a4020747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UserModel         ', 'GroupModel        ', 'MembershipModel   ']\n",
      "['UserGQLModel      ', 'GroupGQLModel     ', 'MembershipGQLModel']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Padding\n",
    "\n",
    "# A simple loop that loops through the list of sentences and adds a ' ' whitespace until the length of the sentence matches\n",
    "# the length of the longest sentence\n",
    "for i in range(len(keys)):\n",
    "    while len(keys[i])<maxlen:\n",
    "        keys[i] += ' '\n",
    "\n",
    "for i in range(len(values)):\n",
    "    while len(values[i])<maxlen:\n",
    "        values[i] += ' '\n",
    "\n",
    "print(keys)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6adcf221-7551-4d90-8459-1819eec30503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: UserModel        \n",
      "Target Sequence: serGQLModel      \n",
      "Input Sequence: GroupModel       \n",
      "Target Sequence: roupGQLModel     \n",
      "Input Sequence: MembershipModel  \n",
      "Target Sequence: embershipGQLModel\n"
     ]
    }
   ],
   "source": [
    "# Creating lists that will hold our input and target sequences\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(keys)):\n",
    "    # Remove last character for input sequence\n",
    "    input_seq.append(keys[i][:-1])\n",
    "    \n",
    "    # Remove firsts character for target sequence\n",
    "    target_seq.append(values[i][1:])\n",
    "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33704a44-cc15-4219-9f36-ffc3304fc2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "84d7e0e0-93f1-4035-be8e-c62b3c343539",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4612ab-8df3-45bb-b2bc-0bcec65e1013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a9b82f6c-78f0-400e-964d-a7d90acc3e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(keys)\n",
    "\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51b1ea1-5544-4347-bd4e-f759b9bc8477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7b026c2f-c489-47b3-bcef-9b2b1172644a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (3, 17, 65) --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n"
     ]
    }
   ],
   "source": [
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
    "print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))\n",
    "#print(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccda063-5c73-4f9b-8e1b-1a4399824fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "60b243f7-e619-425d-b4da-7b871e23aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = torch.from_numpy(input_seq)\n",
    "target_seq = torch.Tensor(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba0555-ec57-40d2-a21c-8ae5e8ac4d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc5ba035-39d5-48fe-8899-059ee06e828d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63171d06-1a4c-4b8d-b372-0b6d31a6112a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "32bb48e3-47d1-407f-9f7f-22e1fdb2e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        #Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910aacd-1363-474e-a004-a73d8f91091b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "67ef5901-5dde-402a-9c24-1f1e4b662377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define hyperparameters\n",
    "n_epochs = 100\n",
    "lr = 0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a847116-e1d6-4f27-ad4c-5deb39ea5a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "40ff7388-c240-42c6-b10e-e52270340f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100............. Loss: 3.1955\n",
      "Epoch: 20/100............. Loss: 2.5670\n",
      "Epoch: 30/100............. Loss: 2.1579\n",
      "Epoch: 40/100............. Loss: 1.7276\n",
      "Epoch: 50/100............. Loss: 1.3123\n",
      "Epoch: 60/100............. Loss: 0.9196\n",
      "Epoch: 70/100............. Loss: 0.6061\n",
      "Epoch: 80/100............. Loss: 0.3866\n",
      "Epoch: 90/100............. Loss: 0.2486\n",
      "Epoch: 100/100............. Loss: 0.1665\n"
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "input_seq = input_seq.to(device)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    #input_seq = input_seq.to(device)\n",
    "    output, hidden = model(input_seq)\n",
    "    output = output.to(device)\n",
    "    target_seq = target_seq.to(device)\n",
    "    loss = criterion(output, target_seq.view(-1).long())\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33caa5d-6e50-44e4-a3f6-2ff0019884d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9e839922-1fca-4e5b-916f-22c9b6b1c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, character):\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    character = np.array([[char2int[c] for c in character]])\n",
    "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
    "    character = torch.from_numpy(character)\n",
    "    character = character.to(device)\n",
    "    \n",
    "    out, hidden = model(character)\n",
    "\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    # Taking the class with the highest probability score from the output\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "    return int2char[char_ind], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531e786c-5dc9-46c6-b49e-bb4cee054607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5916eb9e-f702-4117-ab0d-40284226236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, out_len, start='hey'):\n",
    "    model.eval() # eval mode\n",
    "    start = start.lower()\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(model, chars)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8118eea-73b6-45a1-b061-cdbc34635dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "71b4ac6a-6079-45c6-b336-959a303a358f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userGeoLershirG\n",
      "groupGoLeo\n",
      "memserGGGGGGGGGGGGGG\n",
      "usermodeldeoupGoLeou\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, 15, 'User'))\n",
    "print(sample(model, 10, 'Group'))\n",
    "print(sample(model, 20, 'Mem'))\n",
    "print(sample(model, 20, 'UserModel'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4185c1ff-a0a4-4a18-84e9-1ec52c6801fa",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=aOEoxyA4uXU\n",
    "\n",
    "https://github.com/AssemblyAI-Examples/Machine-Learning-From-Scratch/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b7ba7-4819-4417-9401-f68af4c3c950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
