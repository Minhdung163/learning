{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5485a0b-91b1-42e0-a55c-e9a4e8efc4ba",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db96e7-2ae0-418d-a822-db7edb007711",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a1d4d20-9371-45de-812a-3df95342cdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = (\n",
    "    \"mps\"\n",
    "    if getattr(torch, \"has_mps\", False)\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d85da0-ec15-4bd0-928c-34cd30ad283e",
   "metadata": {},
   "source": [
    "https://github.com/jeffheaton/app_deep_learning/blob/main/t81_558_class_04_1_kfold.ipynb\n",
    "\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab71bc5-9b76-440f-8f31-30a4d55a6d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3] [0 1]\n",
      "[0 1] [2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = [\"a\", \"b\", \"c\", \"d\"]\n",
    "kf = KFold(n_splits=2)\n",
    "for train, test in kf.split(X):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de8ed27-fea0-465b-a9a2-4c1eeb72d804",
   "metadata": {},
   "source": [
    "## EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cddc2df5-a4f0-4cf9-977c-1702b3f03500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping (see module 3.4)\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_model = None\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.status = \"\"\n",
    "\n",
    "    def __call__(self, model, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "        elif self.best_loss - val_loss >= self.min_delta:\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.status = f\"Improvement found, counter reset to {self.counter}\"\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.status = f\"No improvement in the last {self.counter} epochs\"\n",
    "            if self.counter >= self.patience:\n",
    "                self.status = f\"Early stopping triggered after {self.counter} epochs.\"\n",
    "                if self.restore_best_weights:\n",
    "                    model.load_state_dict(self.best_model)\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec7649-a803-4a33-bcc7-57bc0ee929a8",
   "metadata": {},
   "source": [
    "## K-Fold "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1720551-be13-460c-a094-76416eb60111",
   "metadata": {},
   "source": [
    "K-Fold je metoda rozdělení a následné použití trénovacích dat. \n",
    "K-Fold rozděluje data do k skupin. Pro každou skupinu je vytvořen model, který je na této skupině trénován.\n",
    "Zbylé skupiny jsou pro daný model použity jako testovací množina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7765e853-5528-4ef5-8e49-fc1483c9e333",
   "metadata": {},
   "source": [
    "## Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad14111-16ed-4071-a337-457e1b22614c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c656518-a0e2-4961-ab9e-6ede92067ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed332d11-bb61-4383-a0b0-d7b3b9e7b197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666667, 1.        , 0.96666667, 0.96666667, 1.        ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d4af0e-c603-417a-8f52-477aa216748e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "038dd187-0fae-4c2f-831d-52b7c8dfff1e",
   "metadata": {},
   "source": [
    "## Unknown Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e271d809-8d12-42af-a089-be8450d4abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "serverpath = \"https://needtoknow.cz\"\n",
    "document = \"/~profesor/data/jh-simple-dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64737dc1-81f8-47ce-bb38-614cf7eea2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "server = getpass.getpass()\n",
    "serverpath = f\"https://{server}.cz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4099ae2d-2252-453a-b17d-b81307c609d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullurl = serverpath + document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe289a95-f550-4f23-9cf9-26e75eec6df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job</th>\n",
       "      <th>area</th>\n",
       "      <th>income</th>\n",
       "      <th>aspect</th>\n",
       "      <th>subscriptions</th>\n",
       "      <th>dist_healthy</th>\n",
       "      <th>save_rate</th>\n",
       "      <th>dist_unhealthy</th>\n",
       "      <th>age</th>\n",
       "      <th>pop_dense</th>\n",
       "      <th>retail_dense</th>\n",
       "      <th>crime</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>vv</td>\n",
       "      <td>c</td>\n",
       "      <td>50876.0</td>\n",
       "      <td>13.100000</td>\n",
       "      <td>1</td>\n",
       "      <td>9.017895</td>\n",
       "      <td>35</td>\n",
       "      <td>11.738935</td>\n",
       "      <td>49</td>\n",
       "      <td>0.885827</td>\n",
       "      <td>0.492126</td>\n",
       "      <td>0.071100</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>kd</td>\n",
       "      <td>c</td>\n",
       "      <td>60369.0</td>\n",
       "      <td>18.625000</td>\n",
       "      <td>2</td>\n",
       "      <td>7.766643</td>\n",
       "      <td>59</td>\n",
       "      <td>6.805396</td>\n",
       "      <td>51</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.342520</td>\n",
       "      <td>0.400809</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>pe</td>\n",
       "      <td>c</td>\n",
       "      <td>55126.0</td>\n",
       "      <td>34.766667</td>\n",
       "      <td>1</td>\n",
       "      <td>3.632069</td>\n",
       "      <td>6</td>\n",
       "      <td>13.671772</td>\n",
       "      <td>44</td>\n",
       "      <td>0.944882</td>\n",
       "      <td>0.724409</td>\n",
       "      <td>0.207723</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>c</td>\n",
       "      <td>51690.0</td>\n",
       "      <td>15.808333</td>\n",
       "      <td>1</td>\n",
       "      <td>5.372942</td>\n",
       "      <td>16</td>\n",
       "      <td>4.333286</td>\n",
       "      <td>50</td>\n",
       "      <td>0.889764</td>\n",
       "      <td>0.444882</td>\n",
       "      <td>0.361216</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>kl</td>\n",
       "      <td>d</td>\n",
       "      <td>28347.0</td>\n",
       "      <td>40.941667</td>\n",
       "      <td>3</td>\n",
       "      <td>3.822477</td>\n",
       "      <td>20</td>\n",
       "      <td>5.967121</td>\n",
       "      <td>38</td>\n",
       "      <td>0.744094</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.068033</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1996</td>\n",
       "      <td>vv</td>\n",
       "      <td>c</td>\n",
       "      <td>51017.0</td>\n",
       "      <td>38.233333</td>\n",
       "      <td>1</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>34</td>\n",
       "      <td>14.013489</td>\n",
       "      <td>41</td>\n",
       "      <td>0.881890</td>\n",
       "      <td>0.744094</td>\n",
       "      <td>0.104838</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1997</td>\n",
       "      <td>kl</td>\n",
       "      <td>d</td>\n",
       "      <td>26576.0</td>\n",
       "      <td>33.358333</td>\n",
       "      <td>2</td>\n",
       "      <td>3.632069</td>\n",
       "      <td>20</td>\n",
       "      <td>8.380497</td>\n",
       "      <td>38</td>\n",
       "      <td>0.944882</td>\n",
       "      <td>0.877953</td>\n",
       "      <td>0.063851</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1998</td>\n",
       "      <td>kl</td>\n",
       "      <td>d</td>\n",
       "      <td>28595.0</td>\n",
       "      <td>39.425000</td>\n",
       "      <td>3</td>\n",
       "      <td>7.168218</td>\n",
       "      <td>99</td>\n",
       "      <td>4.626950</td>\n",
       "      <td>36</td>\n",
       "      <td>0.759843</td>\n",
       "      <td>0.744094</td>\n",
       "      <td>0.098703</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1999</td>\n",
       "      <td>qp</td>\n",
       "      <td>c</td>\n",
       "      <td>67949.0</td>\n",
       "      <td>5.733333</td>\n",
       "      <td>0</td>\n",
       "      <td>8.936292</td>\n",
       "      <td>26</td>\n",
       "      <td>3.281439</td>\n",
       "      <td>46</td>\n",
       "      <td>0.909449</td>\n",
       "      <td>0.598425</td>\n",
       "      <td>0.117803</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>2000</td>\n",
       "      <td>pe</td>\n",
       "      <td>c</td>\n",
       "      <td>61467.0</td>\n",
       "      <td>16.891667</td>\n",
       "      <td>0</td>\n",
       "      <td>4.312097</td>\n",
       "      <td>8</td>\n",
       "      <td>9.405648</td>\n",
       "      <td>48</td>\n",
       "      <td>0.925197</td>\n",
       "      <td>0.539370</td>\n",
       "      <td>0.451973</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id job area   income     aspect  subscriptions  dist_healthy  \\\n",
       "0        1  vv    c  50876.0  13.100000              1      9.017895   \n",
       "1        2  kd    c  60369.0  18.625000              2      7.766643   \n",
       "2        3  pe    c  55126.0  34.766667              1      3.632069   \n",
       "3        4  11    c  51690.0  15.808333              1      5.372942   \n",
       "4        5  kl    d  28347.0  40.941667              3      3.822477   \n",
       "...    ...  ..  ...      ...        ...            ...           ...   \n",
       "1995  1996  vv    c  51017.0  38.233333              1      5.454545   \n",
       "1996  1997  kl    d  26576.0  33.358333              2      3.632069   \n",
       "1997  1998  kl    d  28595.0  39.425000              3      7.168218   \n",
       "1998  1999  qp    c  67949.0   5.733333              0      8.936292   \n",
       "1999  2000  pe    c  61467.0  16.891667              0      4.312097   \n",
       "\n",
       "      save_rate  dist_unhealthy  age  pop_dense  retail_dense     crime  \\\n",
       "0            35       11.738935   49   0.885827      0.492126  0.071100   \n",
       "1            59        6.805396   51   0.874016      0.342520  0.400809   \n",
       "2             6       13.671772   44   0.944882      0.724409  0.207723   \n",
       "3            16        4.333286   50   0.889764      0.444882  0.361216   \n",
       "4            20        5.967121   38   0.744094      0.661417  0.068033   \n",
       "...         ...             ...  ...        ...           ...       ...   \n",
       "1995         34       14.013489   41   0.881890      0.744094  0.104838   \n",
       "1996         20        8.380497   38   0.944882      0.877953  0.063851   \n",
       "1997         99        4.626950   36   0.759843      0.744094  0.098703   \n",
       "1998         26        3.281439   46   0.909449      0.598425  0.117803   \n",
       "1999          8        9.405648   48   0.925197      0.539370  0.451973   \n",
       "\n",
       "     product  \n",
       "0          b  \n",
       "1          c  \n",
       "2          b  \n",
       "3          b  \n",
       "4          a  \n",
       "...      ...  \n",
       "1995       b  \n",
       "1996       a  \n",
       "1997       f  \n",
       "1998       c  \n",
       "1999       c  \n",
       "\n",
       "[2000 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(fullurl, na_values=[\"NA\", \"?\"])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e35ee82c-1ce7-40ac-b084-9c4897139075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   0,    1,    2, ..., 1997, 1998, 1999]), array([  23,   29,   30,   32,   44,   45,   49,   56,   59,   63,   65,\n",
      "         67,   69,   70,   73,   76,   78,   99,  100,  109,  111,  115,\n",
      "        120,  123,  124,  128,  135,  162,  163,  168,  173,  175,  185,\n",
      "        188,  194,  196,  203,  210,  211,  212,  218,  220,  231,  233,\n",
      "        237,  239,  247,  251,  254,  256,  261,  266,  270,  275,  281,\n",
      "        289,  297,  298,  300,  303,  305,  306,  307,  316,  322,  324,\n",
      "        331,  342,  344,  350,  351,  352,  353,  354,  361,  366,  367,\n",
      "        368,  374,  382,  383,  393,  394,  411,  414,  416,  422,  427,\n",
      "        429,  432,  433,  438,  450,  453,  462,  464,  471,  478,  479,\n",
      "        480,  482,  485,  494,  495,  507,  514,  519,  526,  527,  529,\n",
      "        530,  534,  535,  538,  543,  544,  552,  554,  555,  570,  572,\n",
      "        579,  581,  582,  583,  584,  585,  591,  599,  602,  607,  610,\n",
      "        611,  613,  617,  618,  620,  628,  630,  637,  651,  654,  670,\n",
      "        674,  678,  679,  693,  701,  712,  720,  730,  743,  744,  746,\n",
      "        755,  757,  759,  764,  771,  780,  782,  785,  787,  788,  792,\n",
      "        802,  807,  808,  824,  829,  832,  834,  855,  857,  862,  874,\n",
      "        879,  886,  887,  888,  889,  905,  906,  907,  909,  916,  930,\n",
      "        937,  938,  944,  949,  964,  965,  973,  974,  978,  987,  990,\n",
      "        993, 1004, 1027, 1033, 1036, 1041, 1053, 1054, 1063, 1075, 1078,\n",
      "       1080, 1083, 1084, 1089, 1091, 1100, 1103, 1105, 1107, 1116, 1118,\n",
      "       1120, 1134, 1137, 1164, 1173, 1178, 1179, 1182, 1185, 1190, 1193,\n",
      "       1196, 1198, 1223, 1225, 1229, 1233, 1242, 1244, 1245, 1259, 1265,\n",
      "       1273, 1274, 1278, 1284, 1289, 1290, 1292, 1301, 1310, 1313, 1319,\n",
      "       1323, 1324, 1333, 1335, 1345, 1350, 1364, 1370, 1378, 1381, 1384,\n",
      "       1394, 1395, 1412, 1414, 1419, 1423, 1425, 1431, 1433, 1440, 1449,\n",
      "       1450, 1453, 1460, 1464, 1465, 1472, 1473, 1474, 1483, 1487, 1491,\n",
      "       1497, 1502, 1510, 1512, 1518, 1530, 1532, 1537, 1541, 1543, 1544,\n",
      "       1546, 1550, 1551, 1555, 1556, 1562, 1563, 1566, 1567, 1568, 1573,\n",
      "       1592, 1593, 1600, 1605, 1608, 1609, 1610, 1612, 1615, 1618, 1621,\n",
      "       1623, 1624, 1640, 1646, 1647, 1652, 1654, 1658, 1659, 1664, 1666,\n",
      "       1673, 1674, 1675, 1686, 1691, 1696, 1697, 1699, 1710, 1713, 1719,\n",
      "       1728, 1729, 1731, 1736, 1737, 1739, 1741, 1745, 1754, 1756, 1766,\n",
      "       1767, 1782, 1784, 1789, 1794, 1798, 1803, 1810, 1813, 1814, 1818,\n",
      "       1819, 1825, 1831, 1833, 1838, 1847, 1852, 1860, 1872, 1873, 1884,\n",
      "       1893, 1897, 1903, 1905, 1911, 1912, 1916, 1919, 1922, 1924, 1927,\n",
      "       1930, 1931, 1932, 1934, 1937, 1938, 1950, 1959, 1962, 1965, 1966,\n",
      "       1978, 1981, 1987, 1990]))\n",
      "(array([   0,    1,    3, ..., 1996, 1997, 1999]), array([   2,    6,   15,   18,   31,   43,   48,   51,   54,   58,   71,\n",
      "         72,   81,   83,   84,   86,  101,  107,  113,  118,  141,  147,\n",
      "        148,  155,  170,  174,  177,  179,  181,  182,  184,  192,  198,\n",
      "        199,  208,  214,  221,  226,  236,  240,  243,  244,  250,  259,\n",
      "        265,  271,  272,  273,  274,  277,  285,  286,  287,  292,  296,\n",
      "        308,  309,  310,  311,  312,  326,  327,  329,  332,  334,  339,\n",
      "        341,  346,  358,  360,  363,  365,  370,  371,  376,  377,  380,\n",
      "        381,  398,  405,  408,  413,  415,  420,  423,  425,  426,  428,\n",
      "        430,  435,  436,  439,  442,  445,  451,  457,  461,  465,  468,\n",
      "        481,  483,  486,  490,  493,  497,  500,  505,  506,  513,  518,\n",
      "        522,  528,  532,  548,  551,  557,  560,  561,  566,  567,  575,\n",
      "        576,  588,  589,  590,  593,  596,  598,  601,  609,  614,  619,\n",
      "        621,  629,  631,  643,  650,  660,  669,  677,  680,  692,  694,\n",
      "        700,  704,  705,  706,  707,  710,  715,  721,  722,  727,  733,\n",
      "        736,  741,  745,  752,  756,  765,  767,  772,  774,  777,  781,\n",
      "        806,  809,  813,  817,  818,  819,  836,  845,  849,  873,  881,\n",
      "        900,  903,  904,  914,  915,  922,  925,  926,  931,  932,  936,\n",
      "        939,  940,  941,  942,  948,  950,  952,  962,  966,  968,  979,\n",
      "        982,  985,  986,  994,  998,  999, 1006, 1010, 1013, 1018, 1022,\n",
      "       1029, 1034, 1037, 1040, 1047, 1050, 1052, 1055, 1057, 1067, 1068,\n",
      "       1073, 1085, 1087, 1096, 1106, 1110, 1111, 1112, 1121, 1124, 1131,\n",
      "       1132, 1146, 1151, 1160, 1161, 1165, 1176, 1189, 1192, 1200, 1202,\n",
      "       1204, 1206, 1216, 1220, 1221, 1228, 1230, 1231, 1235, 1237, 1239,\n",
      "       1240, 1249, 1251, 1258, 1271, 1272, 1281, 1283, 1293, 1302, 1307,\n",
      "       1309, 1314, 1316, 1318, 1320, 1322, 1326, 1328, 1330, 1334, 1336,\n",
      "       1343, 1347, 1356, 1357, 1359, 1360, 1362, 1368, 1379, 1383, 1385,\n",
      "       1386, 1387, 1389, 1391, 1402, 1405, 1407, 1421, 1422, 1432, 1435,\n",
      "       1441, 1442, 1454, 1455, 1456, 1458, 1461, 1471, 1475, 1476, 1480,\n",
      "       1481, 1493, 1501, 1509, 1511, 1536, 1538, 1549, 1553, 1554, 1558,\n",
      "       1559, 1560, 1564, 1565, 1572, 1578, 1583, 1587, 1594, 1599, 1604,\n",
      "       1607, 1614, 1617, 1619, 1622, 1628, 1629, 1630, 1635, 1637, 1649,\n",
      "       1650, 1651, 1669, 1676, 1677, 1679, 1688, 1701, 1721, 1726, 1738,\n",
      "       1743, 1755, 1761, 1764, 1769, 1771, 1785, 1788, 1800, 1812, 1816,\n",
      "       1821, 1828, 1829, 1832, 1840, 1846, 1857, 1859, 1861, 1866, 1867,\n",
      "       1869, 1874, 1876, 1883, 1898, 1909, 1923, 1935, 1939, 1940, 1943,\n",
      "       1947, 1956, 1957, 1960, 1961, 1972, 1976, 1977, 1979, 1980, 1985,\n",
      "       1989, 1992, 1993, 1998]))\n",
      "(array([   1,    2,    4, ..., 1997, 1998, 1999]), array([   0,    3,    5,    7,   10,   12,   24,   25,   27,   33,   39,\n",
      "         41,   42,   47,   52,   55,   60,   66,   68,   74,   77,   80,\n",
      "         82,   85,   88,   92,   94,   96,   97,  102,  104,  105,  106,\n",
      "        110,  117,  125,  126,  129,  131,  132,  136,  137,  138,  139,\n",
      "        140,  142,  144,  156,  158,  164,  165,  167,  171,  178,  183,\n",
      "        193,  195,  204,  209,  213,  215,  222,  224,  227,  228,  232,\n",
      "        235,  238,  242,  248,  249,  258,  260,  267,  280,  282,  290,\n",
      "        291,  294,  299,  302,  314,  318,  319,  321,  323,  325,  328,\n",
      "        333,  336,  348,  349,  355,  359,  362,  364,  375,  388,  390,\n",
      "        404,  409,  410,  419,  421,  424,  434,  440,  446,  447,  448,\n",
      "        458,  467,  477,  503,  516,  523,  525,  531,  533,  536,  541,\n",
      "        542,  545,  547,  549,  553,  558,  571,  573,  578,  594,  597,\n",
      "        603,  605,  615,  622,  624,  626,  634,  636,  638,  644,  649,\n",
      "        661,  664,  665,  666,  668,  672,  673,  676,  682,  688,  691,\n",
      "        695,  711,  714,  716,  718,  723,  724,  737,  754,  762,  770,\n",
      "        778,  786,  793,  796,  798,  799,  803,  810,  811,  812,  816,\n",
      "        820,  826,  838,  839,  841,  842,  843,  844,  846,  847,  858,\n",
      "        859,  861,  864,  865,  867,  869,  892,  893,  898,  899,  901,\n",
      "        910,  917,  918,  921,  923,  943,  946,  963,  976,  983,  984,\n",
      "        988,  989,  997, 1000, 1001, 1005, 1007, 1009, 1023, 1026, 1030,\n",
      "       1043, 1046, 1049, 1058, 1061, 1074, 1079, 1090, 1094, 1101, 1102,\n",
      "       1108, 1113, 1114, 1117, 1125, 1128, 1133, 1138, 1144, 1157, 1159,\n",
      "       1163, 1169, 1170, 1175, 1177, 1181, 1187, 1208, 1210, 1211, 1222,\n",
      "       1226, 1234, 1247, 1255, 1261, 1262, 1263, 1268, 1280, 1286, 1287,\n",
      "       1288, 1298, 1299, 1303, 1304, 1305, 1315, 1317, 1329, 1331, 1338,\n",
      "       1339, 1340, 1341, 1342, 1344, 1355, 1358, 1361, 1366, 1374, 1375,\n",
      "       1393, 1403, 1406, 1417, 1418, 1427, 1428, 1429, 1436, 1448, 1452,\n",
      "       1457, 1463, 1466, 1468, 1477, 1488, 1492, 1498, 1503, 1505, 1506,\n",
      "       1507, 1514, 1516, 1517, 1521, 1526, 1540, 1547, 1548, 1574, 1576,\n",
      "       1580, 1582, 1586, 1588, 1591, 1606, 1611, 1616, 1620, 1641, 1644,\n",
      "       1645, 1653, 1660, 1667, 1668, 1670, 1671, 1680, 1689, 1694, 1706,\n",
      "       1711, 1716, 1717, 1727, 1730, 1732, 1734, 1740, 1747, 1760, 1763,\n",
      "       1765, 1775, 1780, 1783, 1786, 1791, 1793, 1797, 1799, 1801, 1802,\n",
      "       1804, 1805, 1809, 1815, 1817, 1824, 1834, 1837, 1844, 1850, 1854,\n",
      "       1855, 1856, 1858, 1864, 1865, 1868, 1871, 1896, 1902, 1913, 1920,\n",
      "       1926, 1929, 1933, 1941, 1944, 1945, 1946, 1954, 1963, 1969, 1973,\n",
      "       1974, 1986, 1994, 1995]))\n",
      "(array([   0,    1,    2, ..., 1995, 1996, 1998]), array([   4,    9,   11,   16,   17,   19,   22,   28,   35,   36,   38,\n",
      "         46,   50,   57,   61,   62,   75,   79,   89,   90,   93,  108,\n",
      "        114,  116,  119,  127,  133,  145,  149,  153,  154,  157,  159,\n",
      "        169,  172,  176,  180,  190,  191,  217,  223,  234,  245,  255,\n",
      "        257,  263,  264,  268,  278,  284,  301,  304,  313,  320,  335,\n",
      "        338,  340,  347,  356,  357,  369,  372,  373,  386,  389,  395,\n",
      "        396,  399,  407,  412,  417,  431,  443,  444,  449,  454,  456,\n",
      "        460,  470,  473,  475,  476,  487,  489,  491,  496,  498,  499,\n",
      "        501,  504,  511,  512,  515,  517,  521,  537,  539,  546,  559,\n",
      "        568,  569,  574,  580,  587,  595,  604,  606,  616,  625,  633,\n",
      "        635,  652,  653,  655,  656,  657,  658,  662,  667,  671,  675,\n",
      "        684,  685,  689,  690,  696,  697,  703,  708,  713,  717,  726,\n",
      "        728,  731,  732,  734,  735,  738,  739,  740,  750,  753,  758,\n",
      "        760,  761,  768,  773,  783,  784,  789,  790,  801,  814,  822,\n",
      "        823,  825,  827,  828,  830,  833,  837,  848,  850,  851,  852,\n",
      "        853,  866,  868,  872,  875,  876,  882,  884,  885,  890,  894,\n",
      "        895,  902,  908,  911,  912,  919,  920,  924,  927,  933,  934,\n",
      "        935,  945,  947,  953,  958,  959,  961,  967,  969,  970,  971,\n",
      "        977,  980,  991,  992,  996, 1002, 1003, 1008, 1011, 1015, 1019,\n",
      "       1024, 1031, 1032, 1035, 1039, 1042, 1048, 1062, 1065, 1066, 1069,\n",
      "       1070, 1072, 1077, 1081, 1088, 1092, 1093, 1097, 1098, 1099, 1115,\n",
      "       1119, 1127, 1139, 1140, 1141, 1142, 1145, 1148, 1149, 1150, 1155,\n",
      "       1156, 1166, 1167, 1168, 1172, 1174, 1188, 1191, 1195, 1197, 1199,\n",
      "       1201, 1203, 1205, 1209, 1212, 1213, 1214, 1217, 1219, 1227, 1232,\n",
      "       1236, 1243, 1246, 1252, 1253, 1260, 1269, 1270, 1276, 1277, 1279,\n",
      "       1285, 1295, 1296, 1308, 1311, 1312, 1325, 1346, 1348, 1351, 1352,\n",
      "       1353, 1365, 1373, 1376, 1377, 1380, 1392, 1399, 1401, 1404, 1416,\n",
      "       1420, 1424, 1444, 1446, 1447, 1467, 1469, 1470, 1486, 1489, 1490,\n",
      "       1494, 1504, 1519, 1523, 1524, 1525, 1531, 1535, 1539, 1542, 1545,\n",
      "       1552, 1557, 1561, 1569, 1571, 1575, 1581, 1596, 1601, 1602, 1613,\n",
      "       1625, 1627, 1632, 1639, 1642, 1656, 1657, 1662, 1665, 1672, 1682,\n",
      "       1684, 1690, 1693, 1703, 1708, 1709, 1718, 1720, 1723, 1735, 1742,\n",
      "       1744, 1746, 1749, 1752, 1759, 1774, 1777, 1778, 1781, 1787, 1790,\n",
      "       1807, 1820, 1826, 1827, 1830, 1849, 1853, 1875, 1878, 1879, 1881,\n",
      "       1887, 1889, 1892, 1904, 1907, 1908, 1910, 1914, 1917, 1918, 1921,\n",
      "       1936, 1949, 1953, 1955, 1958, 1964, 1968, 1971, 1975, 1983, 1984,\n",
      "       1988, 1991, 1997, 1999]))\n",
      "(array([   0,    2,    3, ..., 1997, 1998, 1999]), array([   1,    8,   13,   14,   20,   21,   26,   34,   37,   40,   53,\n",
      "         64,   87,   91,   95,   98,  103,  112,  121,  122,  130,  134,\n",
      "        143,  146,  150,  151,  152,  160,  161,  166,  186,  187,  189,\n",
      "        197,  200,  201,  202,  205,  206,  207,  216,  219,  225,  229,\n",
      "        230,  241,  246,  252,  253,  262,  269,  276,  279,  283,  288,\n",
      "        293,  295,  315,  317,  330,  337,  343,  345,  378,  379,  384,\n",
      "        385,  387,  391,  392,  397,  400,  401,  402,  403,  406,  418,\n",
      "        437,  441,  452,  455,  459,  463,  466,  469,  472,  474,  484,\n",
      "        488,  492,  502,  508,  509,  510,  520,  524,  540,  550,  556,\n",
      "        562,  563,  564,  565,  577,  586,  592,  600,  608,  612,  623,\n",
      "        627,  632,  639,  640,  641,  642,  645,  646,  647,  648,  659,\n",
      "        663,  681,  683,  686,  687,  698,  699,  702,  709,  719,  725,\n",
      "        729,  742,  747,  748,  749,  751,  763,  766,  769,  775,  776,\n",
      "        779,  791,  794,  795,  797,  800,  804,  805,  815,  821,  831,\n",
      "        835,  840,  854,  856,  860,  863,  870,  871,  877,  878,  880,\n",
      "        883,  891,  896,  897,  913,  928,  929,  951,  954,  955,  956,\n",
      "        957,  960,  972,  975,  981,  995, 1012, 1014, 1016, 1017, 1020,\n",
      "       1021, 1025, 1028, 1038, 1044, 1045, 1051, 1056, 1059, 1060, 1064,\n",
      "       1071, 1076, 1082, 1086, 1095, 1104, 1109, 1122, 1123, 1126, 1129,\n",
      "       1130, 1135, 1136, 1143, 1147, 1152, 1153, 1154, 1158, 1162, 1171,\n",
      "       1180, 1183, 1184, 1186, 1194, 1207, 1215, 1218, 1224, 1238, 1241,\n",
      "       1248, 1250, 1254, 1256, 1257, 1264, 1266, 1267, 1275, 1282, 1291,\n",
      "       1294, 1297, 1300, 1306, 1321, 1327, 1332, 1337, 1349, 1354, 1363,\n",
      "       1367, 1369, 1371, 1372, 1382, 1388, 1390, 1396, 1397, 1398, 1400,\n",
      "       1408, 1409, 1410, 1411, 1413, 1415, 1426, 1430, 1434, 1437, 1438,\n",
      "       1439, 1443, 1445, 1451, 1459, 1462, 1478, 1479, 1482, 1484, 1485,\n",
      "       1495, 1496, 1499, 1500, 1508, 1513, 1515, 1520, 1522, 1527, 1528,\n",
      "       1529, 1533, 1534, 1570, 1577, 1579, 1584, 1585, 1589, 1590, 1595,\n",
      "       1597, 1598, 1603, 1626, 1631, 1633, 1634, 1636, 1638, 1643, 1648,\n",
      "       1655, 1661, 1663, 1678, 1681, 1683, 1685, 1687, 1692, 1695, 1698,\n",
      "       1700, 1702, 1704, 1705, 1707, 1712, 1714, 1715, 1722, 1724, 1725,\n",
      "       1733, 1748, 1750, 1751, 1753, 1757, 1758, 1762, 1768, 1770, 1772,\n",
      "       1773, 1776, 1779, 1792, 1795, 1796, 1806, 1808, 1811, 1822, 1823,\n",
      "       1835, 1836, 1839, 1841, 1842, 1843, 1845, 1848, 1851, 1862, 1863,\n",
      "       1870, 1877, 1880, 1882, 1885, 1886, 1888, 1890, 1891, 1894, 1895,\n",
      "       1899, 1900, 1901, 1906, 1915, 1925, 1928, 1942, 1948, 1951, 1952,\n",
      "       1967, 1970, 1982, 1996]))\n"
     ]
    }
   ],
   "source": [
    "# Cross-Validate\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kf\n",
    "\n",
    "splitted = kf.split(df[\"product\"])\n",
    "for i in splitted:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd5480e-3f4a-4673-8794-ab8651f64611",
   "metadata": {},
   "source": [
    "## D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0be706e0-51ab-4ff8-b860-50e54d92cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "# Generate dummies for job\n",
    "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\",dtype=int)],axis=1)\n",
    "df.drop('job', axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for area\n",
    "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\",dtype=int)],axis=1)\n",
    "df.drop('area', axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for product\n",
    "df = pd.concat([df,pd.get_dummies(df['product'],prefix=\"product\",dtype=int)],axis=1)\n",
    "df.drop('product', axis=1, inplace=True)\n",
    "\n",
    "# Missing values for income\n",
    "med = df['income'].median()\n",
    "df['income'] = df['income'].fillna(med)\n",
    "\n",
    "# Standardize ranges\n",
    "df['income'] = zscore(df['income'])\n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['save_rate'] = zscore(df['save_rate'])\n",
    "df['subscriptions'] = zscore(df['subscriptions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c74067ee-b456-4562-98f9-394552e36e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Epoch 157/500, Validation Loss: 0.7110837697982788, Early stopping triggered after 5 epochs.\n",
      "Fold #2\n",
      "Epoch 149/500, Validation Loss: 0.4980891942977905, Early stopping triggered after 5 epochs.\n",
      "Fold #3\n",
      "Epoch 151/500, Validation Loss: 0.7317754626274109, Early stopping triggered after 5 epochs.\n",
      "Fold #4\n",
      "Epoch 191/500, Validation Loss: 0.42949116230010986, Early stopping triggered after 5 epochs.\n",
      "Fold #5\n",
      "Epoch 139/500, Validation Loss: 1.2475146055221558, Early stopping triggered after 5 epochs.\n",
      "Fold score (RMSE): 1.1104768514633179\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "x_columns = df.columns.drop(['age', 'id'])\n",
    "x = torch.tensor(df[x_columns].values, dtype=torch.float32, device=device)\n",
    "y = torch.tensor(df['age'].values, dtype=torch.float32, device=device).view(-1, 1)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Cross-Validate\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 10\n",
    "\n",
    "fold = 0\n",
    "for train_idx, test_idx in kf.split(x):\n",
    "    fold += 1\n",
    "    print(f\"Fold #{fold}\")\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # PyTorch DataLoader\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Create the model and optimizer\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(x.shape[1], 20),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(20, 10),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(10, 1)\n",
    "    )\n",
    "    model = torch.compile(model,backend=\"aot_eager\").to(device)\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Early Stopping variables\n",
    "    best_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # Training loop\n",
    "    EPOCHS = 500\n",
    "    epoch = 0\n",
    "    done = False\n",
    "    es = EarlyStopping()\n",
    "\n",
    "    while not done and epoch<EPOCHS:\n",
    "        epoch += 1\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = loss_fn(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(x_test)\n",
    "            val_loss = loss_fn(val_output, y_test)\n",
    "\n",
    "        if es(model, val_loss):\n",
    "            done = True\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}, Validation Loss: \"\n",
    "      f\"{val_loss.item()}, {es.status}\")\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    oos_pred = model(x_test)\n",
    "score = torch.sqrt(loss_fn(oos_pred, y_test)).item()\n",
    "print(f\"Fold score (RMSE): {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf42b2a-1b90-4889-86b9-cae792ebe993",
   "metadata": {},
   "source": [
    "**Příklad**\n",
    "\n",
    "> Natrénujte neuronové sítě pro predikci hodnot a2 a a14. Jako nezávislé proměnné použijte 's3','a8','a9','a10','a11','a12','a13','a15'.\n",
    ">\n",
    "> Použijte https://archive.ics.uci.edu/dataset/27/credit+approval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2b9a45-963f-4047-a418-b2b82466635e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
